{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864e3fcf",
   "metadata": {},
   "source": [
    "##  Transformer Architecture in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4120ee",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da03582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akashsingh/Downloads/Code/Transformer-PyTorch/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "tranformer_model = nn.Transformer(\n",
    "    d_model=512, #dimensionality of model inputs\n",
    "    nhead=8, #numbers of attention heads\n",
    "    num_encoder_layers=6, #number of encoder layers\n",
    "    num_decoder_layers=6, #number of decoder layers\n",
    "    )\n",
    "\n",
    "print(tranformer_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888ab35",
   "metadata": {},
   "source": [
    "### Embedding and Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff99433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math #for sine and cosine functions and square root\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)*math.sqrt(self.d_model)  #scaling by sqrt(d_model) - standard practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f3df54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 15.3231, -26.9085,  17.9909,  ...,  -1.6260,  -6.2211,   7.4548],\n",
      "         [-31.8864,  22.6316, -31.2517,  ...,  41.4395,  31.4917,  -1.4392],\n",
      "         [-13.8347,  21.0569, -21.4930,  ...,  31.8133,  -2.1015, -14.7802],\n",
      "         [ 11.0359,   9.9550, -50.8254,  ...,   7.0253,  -8.0004,  13.0305]],\n",
      "\n",
      "        [[ 25.3953,  14.7083,  38.2098,  ...,  -4.5308,  -1.2147,   2.8883],\n",
      "         [-34.0946,  11.9953,  -1.5688,  ...,  14.2127, -20.3298, -33.7656],\n",
      "         [ 19.7240,  32.0493, -27.7060,  ...,  -6.8598, -33.6846, -11.5936],\n",
      "         [ 29.2914, -13.9668, -16.3890,  ...,  13.3196,   5.1478, -14.3283]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "torch.Size([2, 4, 512])\n"
     ]
    }
   ],
   "source": [
    "# Creating embeddings \n",
    "embedding_layer = InputEmbeddings(vocab_size=10_000, d_model=512)\n",
    "embedded_output = embedding_layer(torch.tensor([[1, 2, 3, 4],[5,6,7,8]])) #example input\n",
    "print(embedded_output)\n",
    "print(embedded_output.shape) #should be (2, 4, 512) - (batch_size, sequence_length, d_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
